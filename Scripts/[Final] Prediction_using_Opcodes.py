import numpy as np
import pandas as pd
pd.options.mode.chained_assignment = None
import os
import hashlib
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix

# The Malware files are named Malw1, Malw2,...,Malw200 and Mal_file1, Mal_file2,..., Mal_file400.
# The Benign files are named from Beni1, Beni2,...,Beni504.
# Some Malware and Benign files were corrupted and couldn't be extracted so, those files were deleted.
# The Below lists (Beni_nos,Malw_nos,Mal_file_nos) contain the numbers of available files. This would make it easier for us to access them later.
# 데이터셋 파일을 로드해서 리스트로 만든다

beni_nos=[]
beni_not_nos=[]
Mal_file_nos=[]
Mal_file_not_nos=[]
Malw_nos=[]
Malw_not_nos=[]
#dataset_dir = "C:/Users/proqk/Downloads/Malware_Analysis/dataset"
# C:\Users\proqk\Downloads\Malware_Analysis에서 실행하기
#dataset_dir = os.path.join(os.getcwd(), 'dataset')
dataset_dir = "C:/Users/proqk/Downloads/Malware_Analysis/dataset"
#print(dataset_dir)

for i in range(1,8940):
    if ((os.path.exists(os.path.join(dataset_dir, 'Malware', 'Mal_file'+str(i)+'.csv')))):
        Mal_file_nos.append(i)
    else:
        Mal_file_not_nos.append(i) 

for i in range(1,951):
    if ((os.path.exists(os.path.join(dataset_dir, 'Benign', 'Beni_file'+str(i)+'.csv')))):
        beni_nos.append(i)
    else:
        beni_not_nos.append(i) 

print(len(Mal_file_nos))
print(len(Mal_file_not_nos))
print(len(beni_nos))
print(len(beni_not_nos))

# The hash value of each Opcode Sequence is generated and is added to hashset_benign(for Benign files) and hashset_malware(for Malware files).
# If the hash value generated by an new file is already present in the hash set, then is a duplicate file and it is not added to the Dataset.
# This will ensure the Dataset consists of unique Opcode Sequences.
# 데이터셋을 3개의 사이트에서 크롤링했으므로, 중복 파일이 있을 수 있다
# benign과 malware 각 opcode 시퀀스의 해시 값을 생성하여 이미 해시 집합에 있는 경우 데이터에 추가하지 않는다

hashset_benign = set()
hashset_malware=set()
def hash_sent(sent):
    return hashlib.md5(sent).hexdigest()

# The "corpus" contains the Opcode sequence of the files. 
# 'y' is the dependent variable. '1' denotes the file is a Malware and '0' denotes the file is Benign.
# corpus 리스트는 파일의 opcode시퀀스를 담는다
# y는 1은 malware, 0은 benign파일을 나타낸다
# 참고: 악성(malicious) 또는 비악성(benign)

corpus=[]
y=[]

# If the file is unique, its Opcode sequence is appended to the 'corpus' and it is classified in 'y'.
# This is done for all the Malware and Benign Files.
# 파일이 고유하면 corpus에 opcode시퀀스 추가하고 y에 분류 결과를 저장한다

import dask.dataframe as dd

#The below two lists are for my reference.
beni_final_nos=[]
beni_dup_nos=[]
for name in beni_nos:
    print("Benign"+str(name))
    csvname1=os.path.join(dataset_dir, 'Benign', 'Beni_file'+str(name)+'.csv')
    
    # Usually this is due to dask's dtype inference failing, and may be fixed by specifying dtypes manually by adding Operand, Unnamed..        
    daskdf=dd.read_csv(csvname1, dtype={'Operand 1': 'object',
                                        'Operand 2': 'object',
                                        'Operand1': 'object',
                                        'Operand2': 'object',
                                        'Operand2': 'object',
                                        'Operand3': 'object',
                                        'Operand 3': 'object',
                                        'Operand 4': 'object',
                                        'Unnamed: 7': 'object',
                                        'Unnamed: 8': 'object',
                                        'Unnamed: 9': 'object',
                                        'Unnamed: 10': 'object',
                                        'Unnamed: 11': 'object',
                                        'Unnamed: 12': 'object',
                                        'Unnamed: 13': 'object',
                                        'Unnamed: 14': 'object',
                                        'Unnamed: 15': 'object',
                                        'Hex_Opcode': 'object',
                                        }, on_bad_lines="warn")

    data1 = daskdf.compute()

    # drop unnamed columns
    data1.drop(data1.columns[data1.columns.astype(str).str.contains('unnamed',case = False)], axis = 1, inplace = True)
    
    opcode=(data1["Opcode"].dropna().tolist())
    opc=" ".join(opcode)
    hashValue = hash_sent(opc.encode('utf-8'))
    if hashValue in hashset_benign:
        beni_dup_nos.append(name)
        continue
    else:
        #print(line.strip('\n'))
        corpus.append(opc)
        hashset_benign.add(hashValue)
        beni_final_nos.append(name)
        y.append(0)


Mal_file_final_nos=[]
Mal_file_dup_nos=[]
for name in Mal_file_nos:
    print("Mal_file " + str(name))
    csvname1 = os.path.join(dataset_dir, 'Malware', 'Mal_file'+str(name)+'.csv')
    
    # Usually this is due to dask's dtype inference failing, and may be fixed by specifying dtypes manually by adding Operand, Unnamed..
    daskdf=dd.read_csv(csvname1, dtype={'Operand 1': 'object',
                                        'Operand 2': 'object',
                                        'Operand1': 'object',
                                        'Operand2': 'object',
                                        'Operand2': 'object',
                                        'Operand3': 'object',
                                        'Operand 3': 'object',
                                        'Operand 4': 'object',
                                        'Unnamed: 7': 'object',
                                        'Unnamed: 8': 'object',
                                        'Unnamed: 9': 'object',
                                        'Unnamed: 10': 'object',
                                        'Unnamed: 11': 'object',
                                        'Unnamed: 12': 'object',
                                        'Unnamed: 13': 'object',
                                        'Unnamed: 14': 'object',
                                        'Unnamed: 15': 'object',
                                        'Hex_Opcode': 'object',
                                        }, on_bad_lines="warn")

    data1 = daskdf.compute()

    # drop unnamed columns
    data1.drop(data1.columns[data1.columns.astype(str).str.contains('unnamed',case = False)], axis = 1, inplace = True)

    # drop NaN opcodes 그리고 list로
    opcode=(data1['Opcode'].dropna().tolist())

    # opcode들을 공백을 넣어 문자열로 합치기
    opc=" ".join(opcode)

    # opcode 시퀀스의 hash
    hashValue = hash_sent(opc.encode('utf-8'))
    if hashValue in hashset_malware: 
        Mal_file_dup_nos.append(name) # 해시가 있다면 not unique
    else: 
        #print(line.strip('\n'))
        corpus.append(opc) # append to the corpus
        hashset_malware.add(hashValue) 
        Mal_file_final_nos.append(name) # malware hashset에 고유하면 malware
        y.append(1)

print(len(corpus))
print(len(Mal_file_final_nos))
print(len(Mal_file_dup_nos))
print(len(beni_final_nos))
print(len(beni_dup_nos))

# The list 'y' is changed to array.
y=np.array(y) 

# CountVectorizer converts the Opcode sequence to a matrix of token counts
# CountVectorizer creates the Dataset to be fed into the Machine Learning Algorithms.
# The N-Gram size is given as parameter.
# X1 consists of Opcode as Column names(N-Gram=1) and either Opcode Count or Opcode Frequency as its row data.
# X2 consists of Opcodes as Column names(N-Gram=2) and either Opcode Count or Opcode Frequency as its row data.
# CountVectorizer로 opcode 시퀀스를 토큰수의 행렬로 변환한다
# 참고로 CountVectorizer는 머신 러닝에 입력할 데이터셋을 생성하는 것이다
# n-gram 크기가 매개변수로 주어진다
# X1은 열 이름이 opcode(1-gram), 행 데이터로는 opcode count 또는 opcode frequency (아래서 나눔)
# X2는 열 이름이 opcode(2-gram), 행 데이터로는 opcode count 또는 opcode frequency
# X3는 opcode(3-gram)
# X4는 opcode(4-gram)

from sklearn.feature_extraction.text import CountVectorizer
cv1 = CountVectorizer()
X1 = cv1.fit_transform(corpus).toarray()
cv2 = CountVectorizer(ngram_range=(2,2))
X2 = cv2.fit_transform(corpus).toarray()
cv3 = CountVectorizer(ngram_range=(3,3))
X3 = cv3.fit_transform(corpus).toarray()
cv4 = CountVectorizer(ngram_range=(4,4))
X4 = cv4.fit_transform(corpus).toarray()

# X11 is the Dataset consisting of Opcode Count with N-Gram=1 as it's data.
# X21 is the Dataset consisting of Opcode Count with N-Gram=2 as it's data.
# They are split into Training and Test Data(Although they are split directly from X1 and X2).
# X11은 N-Gram=1인 Opcode Count 데이터로 구성된 데이터셋
# X21은 N-Gram=2인 Opcode Count 데이터로 구성된 데이터셋
# X31은 N-Gram=3, X41은 4
# 훈련 데이터와 테스트 데이터로 나눔. 비율은 train 75 : test 25

from sklearn.model_selection import train_test_split

X11_train, X11_test, y11_train, y11_test = train_test_split(X1, y, test_size = 0.25, random_state = 0)
X21_train, X21_test, y21_train, y21_test = train_test_split(X2, y, test_size = 0.25, random_state = 0)
X31_train, X31_test, y31_train, y31_test = train_test_split(X3, y, test_size = 0.25, random_state = 0)
X41_train, X41_test, y41_train, y41_test = train_test_split(X4, y, test_size = 0.25, random_state = 0)

# X1 and X2 are converted into Frequency form (Mentioned in more detail in README.md).
# X12 is the Dataset consisting of Opcode Frequency with N-Gram=1 as it's data.
# X22 is the Dataset consisting of Opcode Frequency with N-Gram=2 as it's data.
# They are split into Training and Test Data(Although they are split directly from v1 and v2).
# X1과 X2는 Frequency 형태로 변환한다
# 특정 Opcode의 Opcode Frequency = ((파일에서 특정 Opcode가 발생한 횟수) / (파일에서 Opcode가 발생한 총 횟수)) * 100
# X12은 N-Gram=1인 Opcode Frequency 데이터로 구성된 데이터셋
# X22은 N-Gram=2인 Opcode Frequency 데이터로 구성된 데이터셋
# 훈련 데이터와 테스트 데이터로 나눔. 비율은 train 75 : test 25

v1=np.array(X1).astype(np.float32)
for i in range(len(corpus)):
    s=sum(X1[i])
    v1[i]=((X1[i]/s)*100).astype(np.float32)
    
X12_train, X12_test, y12_train, y12_test = train_test_split(v1, y, test_size = 0.25, random_state = 0)

v2=np.array(X2).astype(np.float32)
for i in range(len(corpus)):
    s=sum(X2[i])
    v2[i]=((X2[i]/s)*100).astype(np.float32)

X22_train, X22_test, y22_train, y22_test = train_test_split(v2, y, test_size = 0.25, random_state = 0)

v3=np.array(X3).astype(np.float32)
for i in range(len(corpus)):
    s=sum(X3[i])
    v3[i]=((X3[i]/s)*100).astype(np.float32)

X32_train, X32_test, y32_train, y32_test = train_test_split(v3, y, test_size = 0.25, random_state = 0)

v4=np.array(X4).astype(np.float32)
for i in range(len(corpus)):
    s=sum(X4[i])
    v4[i]=((X4[i]/s)*100).astype(np.float32)

X42_train, X42_test, y42_train, y42_test = train_test_split(v4, y, test_size = 0.25, random_state = 0)

# All the Datasets are fed into various Machine Learning Classification models.
# The Accuracy and the Confusion Matrix is displayed as Output.

import pickle
from sklearn.externals import joblib
models_dir = os.path.join(os.getcwd(), 'models')

# Model 1: Gaussian Naive Bayes
print("GB \n")

from sklearn.naive_bayes import GaussianNB
classifierGB11 = GaussianNB()
classifierGB11.fit(X11_train, y11_train)
y11_pred = classifierGB11.predict(X11_test)
cm11 = confusion_matrix(y11_test, y11_pred)
print(cm11)
print(accuracy_score(y11_test, y11_pred))
print('\n')

classifierGB12 = GaussianNB()
classifierGB12.fit(X12_train, y12_train)
y12_pred = classifierGB12.predict(X12_test)
cm12 = confusion_matrix(y12_test, y12_pred)
print(cm12)
print(accuracy_score(y12_test, y12_pred))
print('\n')

classifierGB21 = GaussianNB()
classifierGB21.fit(X21_train, y21_train)
y21_pred = classifierGB21.predict(X21_test)
cm21 = confusion_matrix(y21_test, y21_pred)
print(cm21)
print(accuracy_score(y21_test, y21_pred))
print('\n')

classifierGB22 = GaussianNB()
classifierGB22.fit(X22_train, y22_train)
y22_pred = classifierGB22.predict(X22_test)
cm22 = confusion_matrix(y22_test, y22_pred)
print(cm22)
print(accuracy_score(y22_test, y22_pred))
print('\n')

classifierGB31 = GaussianNB()
classifierGB31.fit(X31_train, y31_train)
y31_pred = classifierGB31.predict(X31_test)
cm31 = confusion_matrix(y31_test, y31_pred)
print(cm31)
print(accuracy_score(y31_test, y31_pred))
print('\n')

classifierGB32 = GaussianNB()
classifierGB32.fit(X32_train, y32_train)
y32_pred = classifierGB32.predict(X32_test)
cm32 = confusion_matrix(y32_test, y32_pred)
print(cm32)
print(accuracy_score(y32_test, y32_pred))
print('\n')

classifierGB41 = GaussianNB()
classifierGB41.fit(X41_train, y41_train)
y41_pred = classifierGB41.predict(X41_test)
cm41 = confusion_matrix(y41_test, y41_pred)
print(cm41)
print(accuracy_score(y41_test, y41_pred))
print('\n')

classifierGB42 = GaussianNB()
classifierGB42.fit(X42_train, y42_train)
y42_pred = classifierGB42.predict(X42_test)
cm42 = confusion_matrix(y42_test, y42_pred)
print(cm42)
print(accuracy_score(y42_test, y42_pred))
print('\n')

joblib.dump(classifierGB11, os.path.join(models_dir, 'classifierGB11.pkl'))
joblib.dump(classifierGB12, os.path.join(models_dir, 'classifierGB12.pkl'))
joblib.dump(classifierGB21, os.path.join(models_dir, 'classifierGB21.pkl'))
joblib.dump(classifierGB22, os.path.join(models_dir, 'classifierGB22.pkl'))
joblib.dump(classifierGB31, os.path.join(models_dir, 'classifierGB31.pkl'))
joblib.dump(classifierGB32, os.path.join(models_dir, 'classifierGB32.pkl'))
joblib.dump(classifierGB41, os.path.join(models_dir, 'classifierGB41.pkl'))
joblib.dump(classifierGB42, os.path.join(models_dir, 'classifierGB42.pkl'))

# Model 2: Support Vector Machine (SVC with linear kernel)
print("SVC \n")
from sklearn.svm import SVC

classifiersvc11 = SVC(kernel = 'linear', random_state = 0)
classifiersvc11.fit(X11_train, y11_train)
classifiersvc12= SVC(kernel = 'linear', random_state = 0)
classifiersvc12.fit(X12_train, y12_train)

classifiersvc21 = SVC(kernel = 'linear', random_state = 0)
classifiersvc21.fit(X21_train, y21_train)
classifiersvc22 = SVC(kernel = 'linear', random_state = 0)
classifiersvc22.fit(X22_train, y22_train)

classifiersvc31 = SVC(kernel = 'linear', random_state = 0)
classifiersvc31.fit(X31_train, y31_train)
classifiersvc32 = SVC(kernel = 'linear', random_state = 0)
classifiersvc32.fit(X32_train, y32_train)

classifiersvc41 = SVC(kernel = 'linear', random_state = 0)
classifiersvc41.fit(X41_train, y41_train)
classifiersvc42 = SVC(kernel = 'linear', random_state = 0)
classifiersvc42.fit(X42_train, y42_train)


ysvc11_pred=classifiersvc11.predict(X11_test)
ysvc12_pred=classifiersvc12.predict(X12_test)
ysvc21_pred=classifiersvc21.predict(X21_test)
ysvc22_pred=classifiersvc22.predict(X22_test)
ysvc31_pred=classifiersvc31.predict(X31_test)
ysvc32_pred=classifiersvc32.predict(X32_test)
ysvc41_pred=classifiersvc41.predict(X41_test)
ysvc42_pred=classifiersvc42.predict(X42_test)

print("Detection using Opcode Count with N-Grams=1: \n")
cm11 = confusion_matrix(y11_test, ysvc11_pred)
print(cm11)
print(accuracy_score(y11_test, ysvc11_pred))

print("Detection using Opcode Frequency with N-Grams=1: \n")
cm12 = confusion_matrix(y12_test, ysvc12_pred)
print(cm12)
print(accuracy_score(y12_test, ysvc12_pred))

print("Detection using Opcode Count with N-Grams=2: \n")
cm21 = confusion_matrix(y21_test, ysvc21_pred)
print(cm21)
print(accuracy_score(y21_test, ysvc21_pred))

print("Detection using Opcode Frequency with N-Grams=2: \n")
cm22 = confusion_matrix(y22_test, ysvc22_pred)
print(cm22)
print(accuracy_score(y22_test, ysvc22_pred))

print("Detection using Opcode Count with N-Grams=3: \n")
cm31 = confusion_matrix(y31_test, ysvc31_pred)
print(cm31)
print(accuracy_score(y31_test, ysvc31_pred))

print("Detection using Opcode Frequency with N-Grams=3: \n")
cm32 = confusion_matrix(y32_test, ysvc32_pred)
print(cm32)
print(accuracy_score(y32_test, ysvc32_pred))

print("Detection using Opcode Count with N-Grams=4: \n")
cm41 = confusion_matrix(y41_test, ysvc41_pred)
print(cm41)
print(accuracy_score(y41_test, ysvc41_pred))

print("Detection using Opcode Frequency with N-Grams=4: \n")
cm42 = confusion_matrix(y42_test, ysvc42_pred)
print(cm42)
print(accuracy_score(y42_test, ysvc42_pred))

joblib.dump(classifiersvc11, os.path.join(models_dir, 'classifiersvc11.pkl'))
joblib.dump(classifiersvc12, os.path.join(models_dir, 'classifiersvc12.pkl'))
joblib.dump(classifiersvc21, os.path.join(models_dir, 'classifiersvc21.pkl'))
joblib.dump(classifiersvc22, os.path.join(models_dir, 'classifiersvc22.pkl'))
joblib.dump(classifiersvc31, os.path.join(models_dir, 'classifiersvc31.pkl'))
joblib.dump(classifiersvc32, os.path.join(models_dir, 'classifiersvc32.pkl'))
joblib.dump(classifiersvc41, os.path.join(models_dir, 'classifiersvc41.pkl'))
joblib.dump(classifiersvc42, os.path.join(models_dir, 'classifiersvc42.pkl'))

# Model 3: Random Forest
print("RF \n")
from sklearn.ensemble import RandomForestClassifier

classifierrf11 = RandomForestClassifier(n_estimators = 150, criterion = 'entropy', random_state = 0)
classifierrf11.fit(X11_train, y11_train)
classifierrf12= RandomForestClassifier(n_estimators = 150, criterion = 'entropy', random_state = 0)
classifierrf12.fit(X12_train, y12_train)

classifierrf21 = RandomForestClassifier(n_estimators = 150, criterion = 'entropy', random_state = 0)
classifierrf21.fit(X21_train, y21_train)
classifierrf22 = RandomForestClassifier(n_estimators = 150, criterion = 'entropy', random_state = 0)
classifierrf22.fit(X22_train, y22_train)

classifierrf31 = RandomForestClassifier(n_estimators = 150, criterion = 'entropy', random_state = 0)
classifierrf31.fit(X31_train, y31_train)
classifierrf32 = RandomForestClassifier(n_estimators = 150, criterion = 'entropy', random_state = 0)
classifierrf32.fit(X32_train, y32_train)

classifierrf41 = RandomForestClassifier(n_estimators = 150, criterion = 'entropy', random_state = 0)
classifierrf41.fit(X41_train, y41_train)
classifierrf42 = RandomForestClassifier(n_estimators = 150, criterion = 'entropy', random_state = 0)
classifierrf42.fit(X42_train, y42_train)

yrf11_pred=classifierrf11.predict(X11_test)
yrf12_pred=classifierrf12.predict(X12_test)
yrf21_pred=classifierrf21.predict(X21_test)
yrf22_pred=classifierrf22.predict(X22_test)
yrf31_pred=classifierrf31.predict(X31_test)
yrf32_pred=classifierrf32.predict(X32_test)
yrf41_pred=classifierrf41.predict(X41_test)
yrf42_pred=classifierrf42.predict(X42_test)

print("Detection using Opcode Count with N-Grams=1: \n")
cm11 = confusion_matrix(y11_test, yrf11_pred)
print(cm11)
print(accuracy_score(y11_test, yrf11_pred))
print('\n')

print("Detection using Opcode Frequency with N-Grams=1: \n")
cm12 = confusion_matrix(y12_test, yrf12_pred)
print(cm12)
print(accuracy_score(y12_test, yrf12_pred))
print('\n')

print("Detection using Opcode Count with N-Grams=2: \n")
cm21 = confusion_matrix(y21_test, yrf21_pred)
print(cm21)
print(accuracy_score(y21_test, yrf21_pred))
print('\n')

print("Detection using Opcode Frequency with N-Grams=2: \n")
cm22 = confusion_matrix(y22_test, yrf22_pred)
print(cm22)
print(accuracy_score(y22_test, yrf22_pred))
print('\n')

print("Detection using Opcode Count with N-Grams=3: \n")
cm31 = confusion_matrix(y31_test, yrf31_pred)
print(cm31)
print(accuracy_score(y31_test, yrf31_pred))
print('\n')

print("Detection using Opcode Frequency with N-Grams=3: \n")
cm12 = confusion_matrix(y32_test, yrf32_pred)
print(cm32)
print(accuracy_score(y32_test, yrf32_pred))
print('\n')

print("Detection using Opcode Count with N-Grams=4: \n")
cm41 = confusion_matrix(y41_test, yrf41_pred)
print(cm41)
print(accuracy_score(y41_test, yrf41_pred))
print('\n')

print("Detection using Opcode Frequency with N-Grams=4: \n")
cm42 = confusion_matrix(y42_test, yrf42_pred)
print(cm42)
print(accuracy_score(y42_test, yrf42_pred))
print('\n')

joblib.dump(classifierrf11, os.path.join(models_dir, 'classifierrf11.pkl'))
joblib.dump(classifierrf12, os.path.join(models_dir, 'classifierrf12.pkl'))
joblib.dump(classifierrf21, os.path.join(models_dir, 'classifierrf21.pkl'))
joblib.dump(classifierrf22, os.path.join(models_dir, 'classifierrf22.pkl'))
joblib.dump(classifierrf31, os.path.join(models_dir, 'classifierrf31.pkl'))
joblib.dump(classifierrf32, os.path.join(models_dir, 'classifierrf32.pkl'))
joblib.dump(classifierrf41, os.path.join(models_dir, 'classifierrf41.pkl'))
joblib.dump(classifierrf42, os.path.join(models_dir, 'classifierrf42.pkl'))

# Model 4: Support Vector Machine (SVC with RBF kernel)
print("SVM \n")

classifiersvm11 = SVC(kernel = 'rbf', random_state = 0)
classifiersvm11.fit(X11_train, y11_train)
classifiersvm12= SVC(kernel = 'rbf', random_state = 0)
classifiersvm12.fit(X12_train, y12_train)
classifiersvm21 = SVC(kernel = 'rbf', random_state = 0)
classifiersvm21.fit(X21_train, y21_train)
classifiersvm22 = SVC(kernel = 'rbf', random_state = 0)
classifiersvm22.fit(X22_train, y22_train)
classifiersvm31 = SVC(kernel = 'rbf', random_state = 0)
classifiersvm31.fit(X31_train, y31_train)
classifiersvm32 = SVC(kernel = 'rbf', random_state = 0)
classifiersvm32.fit(X32_train, y32_train)
classifiersvm41 = SVC(kernel = 'rbf', random_state = 0)
classifiersvm41.fit(X41_train, y41_train)
classifiersvm42 = SVC(kernel = 'rbf', random_state = 0)
classifiersvm42.fit(X42_train, y42_train)

ysvm11_pred=classifiersvm11.predict(X11_test)
ysvm12_pred=classifiersvm12.predict(X12_test)
ysvm21_pred=classifiersvm21.predict(X21_test)
ysvm22_pred=classifiersvm22.predict(X22_test)
ysvm31_pred=classifiersvm31.predict(X31_test)
ysvm32_pred=classifiersvm32.predict(X32_test)
ysvm41_pred=classifiersvm41.predict(X41_test)
ysvm42_pred=classifiersvm42.predict(X42_test)

print("Detection with N-Grams=1: \n")
cm11 = confusion_matrix(y11_test, ysvm11_pred)
print(cm11)
print(accuracy_score(y11_test, ysvm11_pred))
print('\n')
cm12 = confusion_matrix(y12_test, ysvm12_pred)
print(cm12)
print(accuracy_score(y12_test, ysvm12_pred))
print('\n')

print("Detection with N-Grams=2: \n")
cm21 = confusion_matrix(y21_test, ysvm21_pred)
print(cm21)
print(accuracy_score(y21_test, ysvm21_pred))
print('\n')
cm22 = confusion_matrix(y22_test, ysvm22_pred)
print(cm22)
print(accuracy_score(y22_test, ysvm22_pred))
print('\n')

print("Detection with N-Grams=3: \n")
cm31 = confusion_matrix(y31_test, ysvm31_pred)
print(cm31)
print(accuracy_score(y31_test, ysvm31_pred))
print('\n')
cm32 = confusion_matrix(y32_test, ysvm32_pred)
print(cm32)
print(accuracy_score(y32_test, ysvm32_pred))
print('\n')

print("Detection with N-Grams=4: \n")
cm41 = confusion_matrix(y41_test, ysvm41_pred)
print(cm41)
print(accuracy_score(y41_test, ysvm41_pred))
print('\n')
cm42 = confusion_matrix(y42_test, ysvm42_pred)
print(cm42)
print(accuracy_score(y42_test, ysvm42_pred))
print('\n')

joblib.dump(classifiersvm11, os.path.join(models_dir, 'classifiersvm11.pkl'))
joblib.dump(classifiersvm12, os.path.join(models_dir, 'classifiersvm12.pkl'))
joblib.dump(classifiersvm21, os.path.join(models_dir, 'classifiersvm21.pkl'))
joblib.dump(classifiersvm22, os.path.join(models_dir, 'classifiersvm22.pkl'))
joblib.dump(classifiersvm31, os.path.join(models_dir, 'classifiersvm31.pkl'))
joblib.dump(classifiersvm32, os.path.join(models_dir, 'classifiersvm32.pkl'))
joblib.dump(classifiersvm41, os.path.join(models_dir, 'classifiersvm41.pkl'))
joblib.dump(classifiersvm42, os.path.join(models_dir, 'classifiersvm42.pkl'))

# Model 5: Decision Tree
print("DT \n")
from sklearn.tree import DecisionTreeClassifier

classifierdt11 =DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
classifierdt11.fit(X11_train, y11_train)
classifierdt12= DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
classifierdt12.fit(X12_train, y12_train)

classifierdt21 = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
classifierdt21.fit(X21_train, y21_train)
classifierdt22 = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
classifierdt22.fit(X22_train, y22_train)

classifierdt31 = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
classifierdt31.fit(X31_train, y31_train)
classifierdt32 = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
classifierdt32.fit(X32_train, y32_train)

classifierdt41 = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
classifierdt41.fit(X41_train, y41_train)
classifierdt42 = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
classifierdt42.fit(X42_train, y42_train)

ydt11_pred=classifierdt11.predict(X11_test)
ydt12_pred=classifierdt12.predict(X12_test)
ydt21_pred=classifierdt21.predict(X21_test)
ydt22_pred=classifierdt22.predict(X22_test)
ydt31_pred=classifierdt31.predict(X31_test)
ydt32_pred=classifierdt32.predict(X32_test)
ydt41_pred=classifierdt41.predict(X41_test)
ydt42_pred=classifierdt42.predict(X42_test)

print("Detection with N-Grams=1: \n")
cm11 = confusion_matrix(y11_test, ydt11_pred)
print(cm11)
print(accuracy_score(y11_test, ydt11_pred))
print('\n')
cm12 = confusion_matrix(y12_test, ydt12_pred)
print(cm12)
print(accuracy_score(y12_test, ydt12_pred))
print('\n')

print("Detection with N-Grams=2: \n")
cm21 = confusion_matrix(y21_test, ydt21_pred)
print(cm21)
print(accuracy_score(y21_test, ydt21_pred))
print('\n')
cm22 = confusion_matrix(y22_test, ydt22_pred)
print(cm22)
print(accuracy_score(y22_test, ydt22_pred))
print('\n')

print("Detection with N-Grams=3: \n")
cm31 = confusion_matrix(y31_test, ydt31_pred)
print(cm31)
print(accuracy_score(y31_test, ydt31_pred))
print('\n')
cm32 = confusion_matrix(y32_test, ydt32_pred)
print(cm32)
print(accuracy_score(y32_test, ydt32_pred))
print('\n')

print("Detection with N-Grams=4: \n")
cm41 = confusion_matrix(y41_test, ydt41_pred)
print(cm41)
print(accuracy_score(y41_test, ydt41_pred))
print('\n')
cm42 = confusion_matrix(y42_test, ydt42_pred)
print(cm42)
print(accuracy_score(y42_test, ydt42_pred))
print('\n')

joblib.dump(classifierdt11, os.path.join(models_dir, 'classifierdt11.pkl'))
joblib.dump(classifierdt12, os.path.join(models_dir, 'classifierdt12.pkl'))
joblib.dump(classifierdt21, os.path.join(models_dir, 'classifierdt21.pkl'))
joblib.dump(classifierdt22, os.path.join(models_dir, 'classifierdt22.pkl'))
joblib.dump(classifierdt31, os.path.join(models_dir, 'classifierdt31.pkl'))
joblib.dump(classifierdt32, os.path.join(models_dir, 'classifierdt32.pkl'))
joblib.dump(classifierdt41, os.path.join(models_dir, 'classifierdt41.pkl'))
joblib.dump(classifierdt42, os.path.join(models_dir, 'classifierdt42.pkl'))

# Model 6: K-Nearest Neighbor
print("KNN \n")
from sklearn.neighbors import KNeighborsClassifier

classifierknn11 =KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
classifierknn11.fit(X11_train, y11_train)
classifierknn12= KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
classifierknn12.fit(X12_train, y12_train)

classifierknn21 =KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
classifierknn21.fit(X21_train, y21_train)
classifierknn22 =KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
classifierknn22.fit(X22_train, y22_train)

classifierknn31 =KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
classifierknn31.fit(X31_train, y31_train)
classifierknn32 =KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
classifierknn32.fit(X42_train, y42_train)

classifierknn41 =KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
classifierknn41.fit(X41_train, y41_train)
classifierknn42 =KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
classifierknn42.fit(X42_train, y42_train)

yknn11_pred=classifierknn11.predict(X11_test)
yknn12_pred=classifierknn12.predict(X12_test)
yknn21_pred=classifierknn21.predict(X21_test)
yknn22_pred=classifierknn22.predict(X22_test)
yknn31_pred=classifierknn31.predict(X31_test)
yknn32_pred=classifierknn32.predict(X32_test)
yknn41_pred=classifierknn41.predict(X41_test)
yknn42_pred=classifierknn42.predict(X42_test)

print("Detection with N-Grams=1: \n")
cm11 = confusion_matrix(y11_test, yknn11_pred)
print(cm11)
print(accuracy_score(y11_test, yknn11_pred))
print('\n')
cm12 = confusion_matrix(y12_test, yknn12_pred)
print(cm12)
print(accuracy_score(y12_test, yknn12_pred))
print('\n')

print("Detection with N-Grams=2: \n")
cm21 = confusion_matrix(y21_test, yknn21_pred)
print(cm21)
print(accuracy_score(y21_test, yknn21_pred))
print('\n')
cm22 = confusion_matrix(y22_test, yknn22_pred)
print(cm22)
print(accuracy_score(y22_test, yknn22_pred))
print('\n')

print("Detection with N-Grams=3: \n")
cm31 = confusion_matrix(y31_test, yknn31_pred)
print(cm31)
print(accuracy_score(y31_test, yknn31_pred))
print('\n')
cm32 = confusion_matrix(y32_test, yknn32_pred)
print(cm32)
print(accuracy_score(y32_test, yknn32_pred))
print('\n')

print("Detection with N-Grams=4: \n")
cm41 = confusion_matrix(y41_test, yknn41_pred)
print(cm41)
print(accuracy_score(y41_test, yknn41_pred))
print('\n')
cm42 = confusion_matrix(y42_test, yknn42_pred)
print(cm42)
print(accuracy_score(y42_test, yknn42_pred))
print('\n')

joblib.dump(classifierknn11, os.path.join(models_dir, 'classifierknn11.pkl'))
joblib.dump(classifierknn12, os.path.join(models_dir, 'classifierknn12.pkl'))
joblib.dump(classifierknn21, os.path.join(models_dir, 'classifierknn21.pkl'))
joblib.dump(classifierknn22, os.path.join(models_dir, 'classifierknn22.pkl'))
joblib.dump(classifierknn31, os.path.join(models_dir, 'classifierknn31.pkl'))
joblib.dump(classifierknn32, os.path.join(models_dir, 'classifierknn32.pkl'))
joblib.dump(classifierknn41, os.path.join(models_dir, 'classifierknn41.pkl'))
joblib.dump(classifierknn42, os.path.join(models_dir, 'classifierknn42.pkl'))